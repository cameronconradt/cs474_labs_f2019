{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "DL_Lab6.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true,
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/wingated/cs474_labs_f2019/blob/master/DL_Lab6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cksgAH12XRjV",
    "colab_type": "text"
   },
   "source": [
    "# Description:\n",
    "For this lab, you will code up the [char-rnn model of Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). This is a recurrent neural network that is trained probabilistically on sequences of characters, and that can then be used to sample new sequences that are like the original.\n",
    "\n",
    "This lab will help you develop several new skills, as well as understand some best practices needed for building large models. In addition, we'll be able to create networks that generate neat text!\n",
    "\n",
    "## There are two parts of this lab:\n",
    "###  1.   Wiring up a basic sequence-to-sequence computation graph\n",
    "###  2.   Implementing your own GRU cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2i_QpSsWG4c",
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "\n",
    "## Part 0: Readings, data loading, and high level training\n",
    "\n",
    "---\n",
    "\n",
    "There is a tutorial here that will help build out scaffolding code, and get an understanding of using sequences in pytorch.\n",
    "\n",
    "**TODO:**\n",
    "\n",
    "**DONE:**\n",
    "\n",
    "* Read the following\n",
    "\n",
    "> * [Pytorch sequence-to-sequence tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)\n",
    "* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "l7bdZWxvJrsx",
    "colab_type": "code",
    "outputId": "f1629005-936c-4244-d58c-b64bc9c0d59b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    }
   },
   "source": [
    "! wget -O ./text_files.tar.gz 'https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz' \n",
    "! tar -xzf text_files.tar.gz\n",
    "! pip install unidecode\n",
    "! pip install torch\n",
    "\n",
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    " \n",
    "import pdb\n",
    " \n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "file = unidecode.unidecode(open('./text_files/lotr.txt').read())\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "--2019-07-11 23:47:28--  https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz\n",
      "Resolving piazza.com (piazza.com)... 3.215.209.172, 54.236.201.50, 34.199.224.99, ...\n",
      "Connecting to piazza.com (piazza.com)|3.215.209.172|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://d1b10bmlvqabco.cloudfront.net/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz [following]\n",
      "--2019-07-11 23:47:33--  https://d1b10bmlvqabco.cloudfront.net/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz\n",
      "Resolving d1b10bmlvqabco.cloudfront.net (d1b10bmlvqabco.cloudfront.net)... 99.86.32.60, 99.86.32.115, 99.86.32.66, ...\n",
      "Connecting to d1b10bmlvqabco.cloudfront.net (d1b10bmlvqabco.cloudfront.net)|99.86.32.60|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1533290 (1.5M) [application/x-gzip]\n",
      "Saving to: ‘./text_files.tar.gz’\n",
      "\n",
      "./text_files.tar.gz 100%[===================>]   1.46M  2.99MB/s    in 0.5s    \n",
      "\n",
      "2019-07-11 23:47:34 (2.99 MB/s) - ‘./text_files.tar.gz’ saved [1533290/1533290]\n",
      "\n",
      "Collecting unidecode\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
      "\u001b[K     |████████████████████████████████| 245kB 5.0MB/s \n",
      "\u001b[?25hInstalling collected packages: unidecode\n",
      "Successfully installed unidecode-1.1.1\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.16.4)\n",
      "file_len = 2579888\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TxBeKeNjJ0NQ",
    "colab_type": "code",
    "outputId": "adc1e200-9e20-4785-b1cd-d5d1800ce6f2",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    }
   },
   "source": [
    "chunk_len = 200\n",
    " \n",
    "def random_chunk():\n",
    "  start_index = random.randint(0, file_len - chunk_len)\n",
    "  end_index = start_index + chunk_len + 1\n",
    "  return file[start_index:end_index]\n",
    "  \n",
    "print(random_chunk())"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      " had not had \n",
      "any burns, luckily. He did not want his folk to hurt themselves in their \n",
      "fury, and he did not want Saruman to escape out of some hole in the \n",
      "confusion. Many of the Ents were hurling the\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "On0_WitWJ99e",
    "colab_type": "code",
    "outputId": "e854deb4-0cb5-4766-817f-8016bcd736c1",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    }
   },
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "  tensor = torch.zeros(len(string)).long()\n",
    "  for c in range(len(string)):\n",
    "      tensor[c] = all_characters.index(string[c])\n",
    "  return Variable(tensor)\n",
    "\n",
    "print(char_tensor('abcDEF'))"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "tensor([10, 11, 12, 39, 40, 41])\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYJPTLcaYmfI",
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "\n",
    "## Part 4: Creating your own GRU cell \n",
    "\n",
    "**(Come back to this later - its defined here so that the GRU will be defined before it is used)**\n",
    "\n",
    "---\n",
    "\n",
    "The cell that you used in Part 1 was a pre-defined Pytorch layer. Now, write your own GRU class using the same parameters as the built-in Pytorch class does.\n",
    "\n",
    "Please try not to look at the GRU cell definition. The answer is right there in the code, and in theory, you could just cut-and-paste it. This bit is on your honor!\n",
    "\n",
    "**TODO:**\n",
    "\n",
    "**DONE:**\n",
    "\n",
    "* Create a custom GRU cell"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aavAv50ZKQ-F",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class GRU(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, num_layers):\n",
    "    super(GRU, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_layers = num_layers\n",
    "    self.reset_input_layers = []\n",
    "    self.reset_hidden_layers = []\n",
    "    self.forget_input_layers = []\n",
    "    self.forget_hidden_layers = []\n",
    "    self.new_input_layers = []\n",
    "    self.new_hidden_layers = []\n",
    "    self.reset_input_layers.append(nn.Linear(input_size, hidden_size))\n",
    "    self.reset_hidden_layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "    self.forget_input_layers.append(nn.Linear(input_size, hidden_size))\n",
    "    self.forget_hidden_layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "    self.new_input_layers.append(nn.Linear(input_size, hidden_size))\n",
    "    self.new_hidden_layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "    for i in range(num_layers -1):\n",
    "      self.reset_input_layers.append(nn.Linear(input_size, hidden_size))\n",
    "      self.reset_hidden_layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "      self.forget_input_layers.append(nn.Linear(input_size, hidden_size))\n",
    "      self.forget_hidden_layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "      self.new_input_layers.append(nn.Linear(input_size, hidden_size))\n",
    "      self.new_hidden_layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "    for i, layer in enumerate(self.reset_input_layers):\n",
    "      self.add_module(str(i) + \"_reset_in\", layer)\n",
    "    for i, layer in enumerate(self.forget_input_layers):\n",
    "      self.add_module(str(i) + \"_forget_in\", layer)\n",
    "    for i, layer in enumerate(self.new_input_layers):\n",
    "      self.add_module(str(i) + \"_new_in\", layer)\n",
    "    for i, layer in enumerate(self.reset_hidden_layers):\n",
    "      self.add_module(str(i) + \"_reset_hidden\", layer)\n",
    "    for i, layer in enumerate(self.forget_hidden_layers):\n",
    "      self.add_module(str(i) + \"_forget_hidden\", layer)\n",
    "    for i, layer in enumerate(self.new_hidden_layers):\n",
    "      self.add_module(str(i) + \"_new_hidden\", layer)\n",
    "    \n",
    "  \n",
    "  def forward(self, inputs, hidden):\n",
    "    # Each layer does the following:\n",
    "    # r_t = sigmoid(W_ir*x_t + b_ir + W_hr*h_(t-1) + b_hr)\n",
    "    # z_t = sigmoid(W_iz*x_t + b_iz + W_hz*h_(t-1) + b_hz)\n",
    "    # n_t = tanh(W_in*x_t + b_in + r_t**(W_hn*h_(t-1) + b_hn))\n",
    "    # h_(t) = (1 - z_t)**n_t + z_t**h_(t-1)\n",
    "    # Where ** is hadamard product (not matrix multiplication, but elementwise multiplication)\n",
    "    hiddens = None\n",
    "    inputs = inputs.view(1,self.input_size)\n",
    "    for i in range(self.num_layers):\n",
    "      new_input = self.new_input_layers[i]\n",
    "      new_hidden = self.new_hidden_layers[i]\n",
    "      reset_input = self.reset_input_layers[i]\n",
    "      reset_hidden = self.reset_hidden_layers[i]\n",
    "      forget_input = self.forget_input_layers[i]\n",
    "      forget_hidden = self.forget_hidden_layers[i]\n",
    "      r_t = torch.sigmoid(reset_input(inputs) + reset_hidden(hidden[i]))\n",
    "      z_t = torch.sigmoid(forget_input(inputs) + forget_hidden(hidden[i]))\n",
    "      n_t = torch.tanh(new_input(inputs) + r_t*(new_hidden(hidden[i])))\n",
    "      outputs = (1-z_t)*n_t + z_t*hidden[i]\n",
    "      if hiddens is None:\n",
    "        hiddens = outputs.unsqueeze(0)\n",
    "      else:\n",
    "        hiddens = torch.cat((hiddens, outputs.unsqueeze(0)), dim=0)\n",
    "      inputs = outputs\n",
    "    return outputs, hiddens\n",
    "  \n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtXdX-B_WiAY",
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "\n",
    "##  Part 1: Building a sequence to sequence model\n",
    "\n",
    "---\n",
    "\n",
    "Great! We have the data in a useable form. We can switch out which text file we are reading from, and trying to simulate.\n",
    "\n",
    "We now want to build out an RNN model, in this section, we will use all built in Pytorch pieces when building our RNN class.\n",
    "\n",
    "\n",
    "**TODO:**\n",
    "\n",
    "**DONE:**\n",
    "\n",
    "* Create an RNN class that extends from nn.Module."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "d6tNdEnzWj5F",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class RNN(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "    super(RNN, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.n_layers = n_layers\n",
    "    # encode using embedding layer\n",
    "    self.encoding = nn.Embedding(input_size, hidden_size) \n",
    "    # set up GRU passing in number of layers parameter (nn.GRU)\n",
    "    self.GRU = GRU(input_size=hidden_size, hidden_size=hidden_size,num_layers=n_layers)\n",
    "    # decode output\n",
    "    self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "  def forward(self, input_char, hidden):\n",
    "    # by reviewing the documentation, construct a forward function that properly uses the output\n",
    "    # of the GRU\n",
    "#     print(input_char)\n",
    "    encoded = self.encoding(input_char.unsqueeze(0).view(-1,1))\n",
    "    out, hidden = self.GRU(encoded, hidden)\n",
    "#     print(out.size())\n",
    "    out_decoded = self.out(out.view(-1,self.hidden_size))\n",
    "    # return output and hidden\n",
    "    return out_decoded, hidden\n",
    "\n",
    "  def init_hidden(self):\n",
    "    return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hrhXghEPKD-5",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def random_training_set():    \n",
    "  chunk = random_chunk()\n",
    "  inp = char_tensor(chunk[:-1])\n",
    "  target = char_tensor(chunk[1:])\n",
    "  return inp, target"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZpiGObbBX0Mr",
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "\n",
    "## Part 2: Sample text and Training information\n",
    "\n",
    "---\n",
    "\n",
    "We now want to be able to train our network, and sample text after training.\n",
    "\n",
    "This function outlines how training a sequence style network goes. \n",
    "\n",
    "**TODO:**\n",
    "\n",
    "**DONE:**\n",
    "\n",
    "* Fill in the pieces.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2ALC3Pf8Kbsi",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def train(inp, target):\n",
    "  ## initialize hidden layers, set up gradient and loss \n",
    "    # your code here\n",
    "  ## /\n",
    "  decoder_optimizer.zero_grad()\n",
    "  hidden = decoder.init_hidden()\n",
    "  loss = 0\n",
    "  for c in range(chunk_len):\n",
    "      output, hidden = decoder(inp[c], hidden)# run the forward pass of your rnn with proper input\n",
    "#       print(output.size())\n",
    "#       print(hidden.size())\n",
    "#       print(target.size())\n",
    "      loss += criterion(output, target[c].unsqueeze(0))\n",
    "      \n",
    "  ## calculate backwards loss and step the optimizer (globally)\n",
    "    # your code here\n",
    "  ## /\n",
    "  loss.backward()\n",
    "  decoder_optimizer.step()\n",
    "\n",
    "  return loss.item() / chunk_len"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EN06NUu3YRlz",
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "\n",
    "## Part 3: Sample text and Training information\n",
    "\n",
    "---\n",
    "\n",
    "You can at this time, if you choose, also write out your train loop boilerplate that samples random sequences and trains your RNN. This will be helpful to have working before writing your own GRU class.\n",
    "\n",
    "If you are finished training, or during training, and you want to sample from the network you may consider using the following function. If your RNN model is instantiated as `decoder`then this will probabilistically sample a sequence of length `predict_len`\n",
    "\n",
    "**TODO:**\n",
    "\n",
    "**DONE:**\n",
    "\n",
    "* Fill out the evaluate function to generate text frome a primed string"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "B-bp-OZ1KjNh",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
    "  ## initialize hidden variable, initialize other useful variables \n",
    "    # your code here\n",
    "  ## /\n",
    "  hidden = decoder.init_hidden()\n",
    "  prime_input = char_tensor(prime_str)\n",
    "\n",
    "  # Use priming string to \"build up\" hidden state\n",
    "  for p in range(len(prime_str) - 1):\n",
    "      _, hidden = decoder(prime_input[p], hidden)\n",
    "  inp = prime_input[-1]\n",
    "\n",
    "  predicted = []\n",
    "  predicted.extend(prime_input)\n",
    "  \n",
    "  for p in range(predict_len):\n",
    "      output, hidden = decoder(inp, hidden)#run your RNN/decoder forward on the input\n",
    "\n",
    "      # Sample from the network as a multinomial distribution\n",
    "      output_dist = output.data.view(-1).div(temperature).exp()\n",
    "      top_i = torch.multinomial(output_dist, 1)[0]\n",
    "\n",
    "      ## get character from your list of all characters, add it to your output str sequence, set input\n",
    "      ## for the next pass through the model\n",
    "       # your code here\n",
    "      ## /\n",
    "      inp = top_i #all_characters[top_i]\n",
    "      \n",
    "      predicted.append(inp)\n",
    "      \n",
    "  predicted = [all_characters[i] for i in predicted]\n",
    "  return ''.join(predicted)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Du4AGA8PcFEW",
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "\n",
    "## Part 4: (Create a GRU cell, requirements above)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFS2bpHSZEU6",
    "colab_type": "text"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Part 5: Run it and generate some text!\n",
    "\n",
    "---\n",
    "\n",
    "Assuming everything has gone well, you should be able to run the main function in the scaffold code, using either your custom GRU cell or the built in layer, and see output something like this. I trained on the “lotr.txt” dataset, using chunk_length=200, hidden_size=100 for 2000 epochs gave.\n",
    "\n",
    "**TODO:** \n",
    "\n",
    "**DONE:**\n",
    "* Create some cool output\n",
    "\n",
    "\n",
    "```\n",
    "[0m 9s (100 5%) 2.2169]\n",
    " Whaiss Mainde \n",
    "\n",
    "'\n",
    "\n",
    "he and the \n",
    "\n",
    "\n",
    "\n",
    "'od and roulll and Are say the \n",
    "rere. \n",
    "'Wor \n",
    "'Iow anond wes ou \n",
    "\n",
    "'Yi \n",
    "\n",
    "[0m 19s (200 10%) 2.0371]\n",
    "Whimbe. \n",
    "\n",
    "'Thhe \n",
    "on not of they was thou hit of \n",
    "sil ubat thith hy the seare \n",
    "as sower and of len beda \n",
    "\n",
    "[0m 29s (300 15%) 2.0051]\n",
    "Whis the cart. Whe courn!' 'Bu't of they aid dou giter of fintard of the not you ous, \n",
    "'Thas orntie it \n",
    "\n",
    "[0m 38s (400 20%) 1.8617]\n",
    "Wh win took be to the know the gost bing to kno wide dought, and he as of they thin. \n",
    "\n",
    "The Gonhis gura \n",
    "\n",
    "[0m 48s (500 25%) 1.9821]\n",
    "When of they singly call the and thave thing \n",
    "they the nowly we'tly by ands, of less be grarmines of t \n",
    "\n",
    "[0m 58s (600 30%) 1.8170]\n",
    "Whinds to mass of I \n",
    "not ken we ting and dour \n",
    "and they. \n",
    "\n",
    "\n",
    "'Wat res swe Ring set shat scmaid. The \n",
    "ha \n",
    "\n",
    "[1m 7s (700 35%) 2.0367]\n",
    "Whad ded troud wanty agy. Ve tanle gour the gone veart on hear, as dent far of the Ridgees.' \n",
    "\n",
    "'The Ri \n",
    "\n",
    "[1m 17s (800 40%) 1.9458]\n",
    "Whis is brouch Heared this lack and was weself, for on't \n",
    "abothom my and go staid it \n",
    "they curse arsh  \n",
    "\n",
    "[1m 27s (900 45%) 1.7522]\n",
    "Whout bear the \n",
    "Evening \n",
    "the pace spood, Arright the spaines beren the and Wish was was on the more yo \n",
    "\n",
    "[1m 37s (1000 50%) 1.6444]\n",
    "Whe Swarn. at colk. N(r)rce or they he \n",
    "wearing. And the on the he was are he said Pipin. \n",
    "\n",
    "'Yes and i \n",
    "\n",
    "[1m 47s (1100 55%) 1.8770]\n",
    "Whing at they and thins the Wil might \n",
    "happened you dlack rusting and thousting fy them, there lifted  \n",
    "\n",
    "[1m 57s (1200 60%) 1.9401]\n",
    "Wh the said Frodo eary him that the herremans! \n",
    "\n",
    "'I the Lager into came and broveener he sanly \n",
    "for \n",
    "s \n",
    "\n",
    "[2m 7s (1300 65%) 1.8095]\n",
    "When lest \n",
    "- in sound fair, and \n",
    "the Did dark he in the gose cilling the stand I in the sight. Frodo y \n",
    "\n",
    "[2m 16s (1400 70%) 1.9229]\n",
    "Whing in a shade and Mowarse round and parse could pass not a have partainly. ' for as I come of I \n",
    "le \n",
    "\n",
    "[2m 26s (1500 75%) 1.8169]\n",
    "Whese one her of in a lief that, \n",
    "but. 'We repagessed, \n",
    "wandere in these fair of long one have here my \n",
    "\n",
    "[2m 36s (1600 80%) 1.6635]\n",
    "Where fread in thougraned in woohis, on the the green the \n",
    "pohered alked tore becaming was seen what c \n",
    "\n",
    "[2m 46s (1700 85%) 1.7868]\n",
    "Whil neat \n",
    "came to \n",
    "is laked, \n",
    "and fourst on him grey now they as pass away aren have in the border sw \n",
    "\n",
    "[2m 56s (1800 90%) 1.6343]\n",
    "Wh magered. \n",
    "\n",
    "Then tell some tame had bear that \n",
    "came as it nome in \n",
    "to houbbirnen and to heardy. \n",
    "\n",
    "\n",
    "' \n",
    "\n",
    "[3m 6s (1900 95%) 1.8191]\n",
    "Who expey to must away be to the master felkly and for, what shours was alons? I had be the long to fo \n",
    "\n",
    "[3m 16s (2000 100%) 1.8725]\n",
    "White, and his of his in before that for brown before can then took on the fainter smass about rifall\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-nXFeCmdKodw",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import time\n",
    "n_epochs = 5000\n",
    "print_every = 200\n",
    "plot_every = 10\n",
    "hidden_size = 200\n",
    "n_layers = 3\n",
    "lr = 0.001\n",
    " \n",
    "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    " \n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xKfozqw-6eqb",
    "colab_type": "code",
    "outputId": "67ef24af-3479-442a-ef08-361386c99d4c",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "# n_epochs = 2000\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  loss_ = train(*random_training_set())       \n",
    "  loss_avg += loss_\n",
    "\n",
    "  if epoch % print_every == 0:\n",
    "      print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
    "      print(evaluate('Wh', 100), '\\n')\n",
    "\n",
    "  if epoch % plot_every == 0:\n",
    "      all_losses.append(loss_avg / plot_every)\n",
    "      loss_avg = 0"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[135.32607412338257 (200 4%) 2.2093]\n",
      "When; \n",
      "And He garn I the. 'werat cawn \n",
      "He mewann the \n",
      "thwanly sheat thind revat we's the mland the't h \n",
      "\n",
      "[270.05828642845154 (400 8%) 1.9820]\n",
      "Whery the over of he hacy there \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "fet, lagound the woll us world war itt aratt the \n",
      "the one. There  \n",
      "\n",
      "[408.5662467479706 (600 12%) 1.9200]\n",
      "White, and there gane houtetle to be that in the string now oof them whe said our bestend now you sill \n",
      "\n",
      "[545.3730459213257 (800 16%) 1.6831]\n",
      "Why him, tall was proad of that his not his with he hopes \n",
      "now to canters you came is many vade while  \n",
      "\n",
      "[683.2958378791809 (1000 20%) 1.6400]\n",
      "What fould like again us a have day stail a are and soothor and -vuch as battight a grow!' I golding w \n",
      "\n",
      "[827.1255896091461 (1200 24%) 1.9501]\n",
      "Whing. Pitcred their saw hour bit of they would and I gather, be canning out at the going this penting \n",
      "\n",
      "[971.8213222026825 (1400 28%) 1.5967]\n",
      "Whet and from filled \n",
      "all the road the pass all the Ent hard \n",
      "not barrers. \n",
      "\n",
      "'And and east there were, \n",
      "\n",
      "[1116.6569247245789 (1600 32%) 1.6283]\n",
      "Where, and \n",
      "first his fire-came of the still fuck the hooken the like. \n",
      "Thusken see strange, and all t \n",
      "\n",
      "[1258.913860797882 (1800 36%) 1.6279]\n",
      "Whin unone I his \n",
      "set and a grown and the bow. So more the sprink and \n",
      "was in the old the old up the G \n",
      "\n",
      "[1401.2823464870453 (2000 40%) 1.4856]\n",
      "Whill new and most felt under alone of the \n",
      "buarm Andor from the stared of the great halt, but a mean  \n",
      "\n",
      "[1540.3706405162811 (2200 44%) 1.3630]\n",
      "What as yet about use from the Marking guard we growed the hilled and \n",
      "first was a stream, but of arm  \n",
      "\n",
      "[1678.6597697734833 (2400 48%) 1.6230]\n",
      "When him. The \n",
      "ling for a great that he creechion could from the King and down red by Swrink before El \n",
      "\n",
      "[1814.600891828537 (2600 52%) 1.4601]\n",
      "Whin. \n",
      "\n",
      "Gimli, and them a shall him bear were the Smy, and them cannot start away to be below. \n",
      "\n",
      "There \n",
      "\n",
      "[1949.199021577835 (2800 56%) 1.4810]\n",
      "Where.' \n",
      "\n",
      "'I may for a mist is you strangered their now to hobbit knew of the enemy. It went they that \n",
      "\n",
      "[2085.439206838608 (3000 60%) 1.2471]\n",
      "While cry. 'We halted his great stood out open of \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Gondor, the black and the Ores in the bear so  \n",
      "\n",
      "[2221.020457983017 (3200 64%) 1.3203]\n",
      "Wheld doubt her. \n",
      "\n",
      "'It was come of the glow know with them at the dark lake not be \n",
      "time, you not more \n",
      "\n",
      "[2355.6336069107056 (3400 68%) 1.3546]\n",
      "Whelf he desive at last came \n",
      "the reached in the back if the western his breach that we statting from  \n",
      "\n",
      "[2489.707965373993 (3600 72%) 1.3227]\n",
      "Whelf. When It was of the will come a choot ows of the \n",
      "words be lines was not ror are had been wise w \n",
      "\n",
      "[2626.3549468517303 (3800 76%) 1.3656]\n",
      "When \n",
      "atching up fire him. And for you'll could be it, le set the morn! \n",
      "\n",
      "'Yes you may have eyes! I ar \n",
      "\n",
      "[2762.682857990265 (4000 80%) 1.3669]\n",
      "What is they \n",
      "seen it. Take your seen from the East rain flat of \n",
      "old Minas Gaff in the Errand across  \n",
      "\n",
      "[2902.935983657837 (4200 84%) 1.4219]\n",
      "Where the end of a so Isengard \n",
      "slowly. Here the West was a finger, into the Ring again away. Men of t \n",
      "\n",
      "[3040.340714454651 (4400 88%) 1.3677]\n",
      "Which the partain \n",
      "was time, and them that night into count of the journey. The \n",
      "Door and the back and \n",
      "\n",
      "[3188.318279504776 (4600 92%) 1.3990]\n",
      "Whose of the back down. \n",
      "\n",
      "'What we remiered in the Elves.' \n",
      "\n",
      "'And we must Sam don't to say that we sie \n",
      "\n",
      "[3334.01224565506 (4800 96%) 1.3395]\n",
      "Which thought \n",
      "come uncough. Then it was know in the day for dazk, my proed back Forest \n",
      "and the day f \n",
      "\n",
      "[3475.044383764267 (5000 100%) 1.3882]\n",
      "Where turn up to \n",
      "mithrising father of long in the Firamar, and there reckon was \n",
      "string. Only dismois \n",
      "\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ee0so6aKJ5L8",
    "colab_type": "code",
    "outputId": "ffd8ff7d-3b1c-42ba-9957-c78363b3154d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "for i in range(10):\n",
    "  start_strings = [\" Th\", \" wh\", \" he\", \" I \", \" ca\", \" G\", \" lo\", \" ra\"]\n",
    "  start = random.randint(0,len(start_strings)-1)\n",
    "  print(start_strings[start])\n",
    "#   all_characters.index(string[c])\n",
    "  print(evaluate(start_strings[start], 200), '\\n')"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      " G\n",
      " Gandalf was decrond. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'All have lord you. Forward the road at least walk this is stuff, and \n",
      "went to the long grey housel-winding and kindled side was a sleep pleasuring, I do long \n",
      "row hrough. In  \n",
      "\n",
      " lo\n",
      " lost death it. \n",
      "\n",
      "'The last of the gatherings and take you,' said Aragorn, shining out of the Gate. \n",
      "\n",
      "'Yes, as you there were remembaused to seen their pass, when? What \n",
      "said here, such seven an the sear \n",
      "\n",
      " lo\n",
      " low, and frod to keepn \n",
      "Came of their most. But here priced doubtless to an Sam up is \n",
      "masters; he left hor as they are looked. And he could now the long to stout in the right fro horseless of \n",
      "the like \n",
      "\n",
      " I \n",
      " I had been the \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "in his eyes with the perushed to lest, if then only the ring and the legended \n",
      "of the less of the long they which as the \n",
      "enders of Orcovered and smood, and the p \n",
      "\n",
      " I \n",
      " I they were not the lord of the hoomes. \n",
      "\n",
      "Home already well from the Elves. And he sat strength, and we \n",
      "housed out of the good of the days to the mountains from his perith. \n",
      "\n",
      "'Yess! Where though as if  \n",
      "\n",
      " Th\n",
      " There yarden \n",
      "you would guard the hoor might. Far and then may was \n",
      "croties, too began to see the drumbred many line \n",
      "and was then hoard walk and they heart, and the chair of the \n",
      "Ents of way, might was \n",
      "\n",
      " G\n",
      " Gandalf \n",
      "been lat of less the round of the stump; both and seemed to the trees and perished they \n",
      "lay are speered the less; and the wind the steep and have to she \n",
      "precious. There was in the oonly went \n",
      "\n",
      " wh\n",
      " which went out of the door. \n",
      "\n",
      "Hull the King and of the The days of his brodo \n",
      "stumbler of the windard was a thing there, then it been shining langing \n",
      "to him poor land. They hands; though they seemed ou \n",
      "\n",
      " ra\n",
      " rather,' have all the least deather \n",
      "down of the truven beginning to the house of sunk. \n",
      "\n",
      "'Nark shorts of the Eyes of the Gate your great nothing as Eret. \n",
      "\n",
      "'I wander trust horn, and there were not, it  \n",
      "\n",
      " I \n",
      " I can have no mind \n",
      "together! Where don't may had one may little blung \n",
      "terrible to tales. And turn and Gandalf shall be not to as only the Cattring \n",
      "not stopped great the out them forms. On they she lo \n",
      "\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJhgDc2IauPE",
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "\n",
    "## Part 6: Generate output on a different dataset\n",
    "\n",
    "---\n",
    "\n",
    "**TODO:**\n",
    "\n",
    "* Choose a textual dataset. Here are some [text datasets](https://www.kaggle.com/datasets?tags=14104-text+data%2C13205-text+mining) from Kaggle \n",
    "\n",
    "* Generate some decent looking results and evaluate your model's performance (say what it did well / not so well)\n",
    "\n",
    "**DONE:**\n",
    "\n"
   ]
  }
 ]
}